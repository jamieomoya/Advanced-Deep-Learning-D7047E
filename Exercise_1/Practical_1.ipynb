{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JNOIFDF29Im7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In0qh9AXB1QZ",
        "outputId": "ffe5b9bd-2c79-4efc-a88c-43eeb75658b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:06<00:00, 26723610.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GWKRpFQICRse"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_leaky_relu = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pBuTZGlUVnJN"
      },
      "outputs": [],
      "source": [
        "def train_model(optimizer, criterion, epochs, model):\n",
        "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')\n",
        "  writer.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fGY3sKgQap2L"
      },
      "outputs": [],
      "source": [
        "def predict_test(model):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = model(images)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY5iel94W738"
      },
      "source": [
        "### Use SGD as Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFuCakKtW3Zm",
        "outputId": "cd9e8bab-22bc-4bb3-f9f5-d8bebde30132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.304\n",
            "[1,  4000] loss: 2.303\n",
            "[1,  6000] loss: 2.304\n",
            "[1,  8000] loss: 2.304\n",
            "[1, 10000] loss: 2.304\n",
            "[1, 12000] loss: 2.303\n",
            "[2,  2000] loss: 2.302\n",
            "[2,  4000] loss: 2.303\n",
            "[2,  6000] loss: 2.303\n",
            "[2,  8000] loss: 2.303\n",
            "[2, 10000] loss: 2.302\n",
            "[2, 12000] loss: 2.301\n",
            "[3,  2000] loss: 2.302\n",
            "[3,  4000] loss: 2.301\n",
            "[3,  6000] loss: 2.301\n",
            "[3,  8000] loss: 2.301\n",
            "[3, 10000] loss: 2.300\n",
            "[3, 12000] loss: 2.300\n",
            "[4,  2000] loss: 2.300\n",
            "[4,  4000] loss: 2.299\n",
            "[4,  6000] loss: 2.299\n",
            "[4,  8000] loss: 2.299\n",
            "[4, 10000] loss: 2.298\n",
            "[4, 12000] loss: 2.298\n",
            "[5,  2000] loss: 2.298\n",
            "[5,  4000] loss: 2.296\n",
            "[5,  6000] loss: 2.297\n",
            "[5,  8000] loss: 2.296\n",
            "[5, 10000] loss: 2.296\n",
            "[5, 12000] loss: 2.295\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 12 %\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_sgd = optim.SGD(model_leaky_relu.parameters(), lr=0.0001)\n",
        "train_model(optimizer_sgd, criterion, 5, model_leaky_relu)\n",
        "predict_test(model_leaky_relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCiyaDm7aNne"
      },
      "source": [
        "### Use ADAM as Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jtbXz5pyUSEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a81d931-f10b-456f-e162-bd1db59a761c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.025\n",
            "[1,  4000] loss: 1.818\n",
            "[1,  6000] loss: 1.716\n",
            "[1,  8000] loss: 1.670\n",
            "[1, 10000] loss: 1.612\n",
            "[1, 12000] loss: 1.566\n",
            "[2,  2000] loss: 1.535\n",
            "[2,  4000] loss: 1.532\n",
            "[2,  6000] loss: 1.514\n",
            "[2,  8000] loss: 1.479\n",
            "[2, 10000] loss: 1.459\n",
            "[2, 12000] loss: 1.447\n",
            "[3,  2000] loss: 1.450\n",
            "[3,  4000] loss: 1.415\n",
            "[3,  6000] loss: 1.393\n",
            "[3,  8000] loss: 1.394\n",
            "[3, 10000] loss: 1.386\n",
            "[3, 12000] loss: 1.365\n",
            "[4,  2000] loss: 1.345\n",
            "[4,  4000] loss: 1.338\n",
            "[4,  6000] loss: 1.317\n",
            "[4,  8000] loss: 1.312\n",
            "[4, 10000] loss: 1.319\n",
            "[4, 12000] loss: 1.314\n",
            "[5,  2000] loss: 1.277\n",
            "[5,  4000] loss: 1.281\n",
            "[5,  6000] loss: 1.258\n",
            "[5,  8000] loss: 1.266\n",
            "[5, 10000] loss: 1.248\n",
            "[5, 12000] loss: 1.253\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 54 %\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_adam = optim.Adam(model_leaky_relu.parameters(), lr=0.0001)\n",
        "train_model(optimizer_adam, criterion, 5, model_leaky_relu)\n",
        "predict_test(model_leaky_relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snWbUqfOmzrI"
      },
      "source": [
        "## Use Tanh as Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yWKKytG_bJtB"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.tanh(self.conv1(x)))\n",
        "        x = self.pool(torch.tanh(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_tanh = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOcYi_DvmzrJ"
      },
      "source": [
        "### Use SGD as Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JK9oVXL4mzrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e228924-76dd-4512-bfa0-12eb5cfd0f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.304\n",
            "[1,  4000] loss: 2.303\n",
            "[1,  6000] loss: 2.303\n",
            "[1,  8000] loss: 2.300\n",
            "[1, 10000] loss: 2.299\n",
            "[1, 12000] loss: 2.297\n",
            "[2,  2000] loss: 2.294\n",
            "[2,  4000] loss: 2.292\n",
            "[2,  6000] loss: 2.288\n",
            "[2,  8000] loss: 2.283\n",
            "[2, 10000] loss: 2.279\n",
            "[2, 12000] loss: 2.273\n",
            "[3,  2000] loss: 2.263\n",
            "[3,  4000] loss: 2.253\n",
            "[3,  6000] loss: 2.242\n",
            "[3,  8000] loss: 2.226\n",
            "[3, 10000] loss: 2.211\n",
            "[3, 12000] loss: 2.198\n",
            "[4,  2000] loss: 2.175\n",
            "[4,  4000] loss: 2.158\n",
            "[4,  6000] loss: 2.142\n",
            "[4,  8000] loss: 2.132\n",
            "[4, 10000] loss: 2.109\n",
            "[4, 12000] loss: 2.097\n",
            "[5,  2000] loss: 2.080\n",
            "[5,  4000] loss: 2.069\n",
            "[5,  6000] loss: 2.058\n",
            "[5,  8000] loss: 2.036\n",
            "[5, 10000] loss: 2.030\n",
            "[5, 12000] loss: 2.025\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 26 %\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_sgd = optim.SGD(model_tanh.parameters(), lr=0.0001)\n",
        "train_model(optimizer_sgd, criterion, 5, model_tanh)\n",
        "predict_test(model_tanh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxX4i0qpmzrJ"
      },
      "source": [
        "### Use ADAM as Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s3SUt0X5mzrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3331348f-d6cd-4424-c219-2175bf2ab03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 1.945\n",
            "[1,  4000] loss: 1.761\n",
            "[1,  6000] loss: 1.661\n",
            "[1,  8000] loss: 1.602\n",
            "[1, 10000] loss: 1.560\n",
            "[1, 12000] loss: 1.505\n",
            "[2,  2000] loss: 1.452\n",
            "[2,  4000] loss: 1.440\n",
            "[2,  6000] loss: 1.425\n",
            "[2,  8000] loss: 1.378\n",
            "[2, 10000] loss: 1.373\n",
            "[2, 12000] loss: 1.381\n",
            "[3,  2000] loss: 1.336\n",
            "[3,  4000] loss: 1.315\n",
            "[3,  6000] loss: 1.332\n",
            "[3,  8000] loss: 1.297\n",
            "[3, 10000] loss: 1.285\n",
            "[3, 12000] loss: 1.276\n",
            "[4,  2000] loss: 1.253\n",
            "[4,  4000] loss: 1.244\n",
            "[4,  6000] loss: 1.228\n",
            "[4,  8000] loss: 1.223\n",
            "[4, 10000] loss: 1.240\n",
            "[4, 12000] loss: 1.200\n",
            "[5,  2000] loss: 1.196\n",
            "[5,  4000] loss: 1.179\n",
            "[5,  6000] loss: 1.161\n",
            "[5,  8000] loss: 1.187\n",
            "[5, 10000] loss: 1.158\n",
            "[5, 12000] loss: 1.162\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 57 %\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_adam = optim.Adam(model_tanh.parameters(), lr=0.0001)\n",
        "train_model(optimizer_adam, criterion, 5, model_tanh)\n",
        "predict_test(model_tanh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dEaFI8NSmzrK"
      },
      "outputs": [],
      "source": [
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}