{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNOIFDF29Im7",
    "outputId": "aaa7ac8c-33f4-4cf5-e4e5-e1386feb346d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models.alexnet as alexnet\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "In0qh9AXB1QZ",
    "outputId": "82a7e3e9-4707-48fb-c041-ced0e2cddd83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:03<00:00, 47468935.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pBuTZGlUVnJN"
   },
   "outputs": [],
   "source": [
    "def train_model(net, optimizer, criterion, epochs):\n",
    "\n",
    "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      # if i <= 2000:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # print(i)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "  print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fGY3sKgQap2L"
   },
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          # calculate outputs by running images through the network\n",
    "          net.to(device)\n",
    "          outputs = net(images)\n",
    "          outputs = outputs.to(device)\n",
    "          # the class with the highest energy is what we choose as prediction\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  #print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "  md(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCiyaDm7aNne"
   },
   "source": [
    "## FINE TUNING with AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtbXz5pyUSEd",
    "outputId": "3ad48756-149b-44e9-c10e-cc031d7ce9a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:04<00:00, 60.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.105\n",
      "[1,  4000] loss: 0.784\n",
      "[1,  6000] loss: 0.697\n",
      "[1,  8000] loss: 0.662\n",
      "[1, 10000] loss: 0.647\n",
      "[1, 12000] loss: 0.623\n",
      "[2,  2000] loss: 0.498\n",
      "[2,  4000] loss: 0.496\n",
      "[2,  6000] loss: 0.516\n",
      "[2,  8000] loss: 0.494\n",
      "[2, 10000] loss: 0.499\n",
      "[2, 12000] loss: 0.510\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# input_size = 224\n",
    "\n",
    "num_classes = 10\n",
    "net = alexnet(pretrained=True)\n",
    "num_ftrs = net.classifier[6].in_features\n",
    "net.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer_adam = optim.Adam(net.parameters(), lr=0.0001)\n",
    "train_model(net, optimizer_adam, criterion, 2)\n",
    "\n",
    "# Predict on test and report Accuracy\n",
    "predict_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmsAEzUamXu9"
   },
   "source": [
    "## FEATURE EXTRACTOR from AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dcq7G2smOaQ",
    "outputId": "e93c2852-96ef-44d7-e15d-a07377e40f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.242\n",
      "[1,  4000] loss: 0.974\n",
      "[1,  6000] loss: 0.921\n",
      "[1,  8000] loss: 0.886\n",
      "[1, 10000] loss: 0.870\n",
      "[1, 12000] loss: 0.865\n",
      "[2,  2000] loss: 0.833\n",
      "[2,  4000] loss: 0.817\n",
      "[2,  6000] loss: 0.818\n",
      "[2,  8000] loss: 0.814\n",
      "[2, 10000] loss: 0.788\n",
      "[2, 12000] loss: 0.800\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# input_size = 224\n",
    "num_classes = 10\n",
    "feature_extract = True\n",
    "net = alexnet(pretrained=True)\n",
    "set_parameter_requires_grad(net, feature_extract)\n",
    "num_ftrs = net.classifier[6].in_features\n",
    "net.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer_adam = optim.Adam(net.parameters(), lr=0.0001)\n",
    "train_model(net, optimizer_adam, criterion, 2)\n",
    "\n",
    "# Predict on test and report Accuracy\n",
    "predict_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USzPGrDHAgi8"
   },
   "source": [
    "### Observation:\n",
    "\n",
    "In all the tests I ran with different number of epochs, the Fine Tuning approach performs much better than the Feature Extractor approach. \n",
    "I also noticed that the Fine Tuning takes longer time than the Feature Extractor approach.\n",
    "\n",
    "I think the difference in performance was because Fine Tuning retrains the layers with the the data we provided, thereby training weights that are more adapted to the new dataset, while the Feature Extractor approach mostly uses weights from the original dataset that the model was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O7U-04voozG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
